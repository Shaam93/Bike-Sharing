{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaam93/Bike-Sharing/blob/master/pytorch_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XVykIbuU0CeU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "First as usual we import the libraries we need for our classifier\n",
        "(we just need to unstall torch and pillow)"
      ]
    },
    {
      "metadata": {
        "id": "4PW58dr6pOKx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Currently the Pytorch version 1.0 is stable and we even have better versions (when I recorded the video only 0.4.0 was stable)\n",
        "#!pip install torch==0.4.0 torchvision\n",
        "!pip install torch torchvision\n",
        "import torch\n",
        "print (\"current pytorch version is: \", torch.__version__)\n",
        "# we need pillow version of 5.3.0\n",
        "import PIL\n",
        "# so we uninstall the older version first\n",
        "!pip uninstall -y Pillow\n",
        "# then install the new one\n",
        "!pip install Pillow==5.3.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRYJdPNcr6sW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import copy\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch import nn, optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms, models, datasets\n",
        "print (\"done importing\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOdDnQ1KnN3I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Getting the flowers data using a URL link"
      ]
    },
    {
      "metadata": {
        "id": "PMvl0ZXoqB3z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget -cq https://github.com/udacity/pytorch_challenge/raw/master/cat_to_name.json\n",
        "!wget -cq https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PHe_UEhDnU6S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Unzipping flower_data.tar.gz"
      ]
    },
    {
      "metadata": {
        "id": "5GrV8LFBql-V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "tar = tarfile.open(\"flower_data.tar.gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AGfBcnRjnae_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Removing flower_data.tar.gz because we now have the unzipped data"
      ]
    },
    {
      "metadata": {
        "id": "uNKbBcLtqtd2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -r flower_data.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R4MbzWEuf9Fw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting up the directories to read the data"
      ]
    },
    {
      "metadata": {
        "id": "CKVoI-Ll-i_1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data_dir = 'flower_data'\n",
        "train_dir = 'train'\n",
        "valid_dir = 'valid'\n",
        "test_dir= 'test'\n",
        "\n",
        "\n",
        "dirs = {'train': train_dir, \n",
        "        'valid': valid_dir, \n",
        "        'test' : test_dir\n",
        "       }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kRqsqIA-gECm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting up the image transforms"
      ]
    },
    {
      "metadata": {
        "id": "gnhZjQys-0jP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# add transforms to the data\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomRotation(45),\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        #transforms.Resize((224, 224)),#attention \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                            [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], \n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H118cXzygJHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Setting up the data loaders"
      ]
    },
    {
      "metadata": {
        "id": "LIFacGK8-3gx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load the datasets with ImageFolder\n",
        "image_datasets = {x: datasets.ImageFolder(dirs[x],   transform=data_transforms[x]) for x in ['train', 'valid', 'test']}\n",
        "# load the data into batches\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16, shuffle=True) for x in ['train', 'valid', 'test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) \n",
        "                              for x in ['train', 'valid', 'test']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TQ2FqaslgRzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Getting the class labels"
      ]
    },
    {
      "metadata": {
        "id": "Zc4sXAfa-7jF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VP_RZrD4gWnL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reading the Json file"
      ]
    },
    {
      "metadata": {
        "id": "BbqvdCwI_ES3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('cat_to_name.json', 'r') as f:\n",
        "    label_map = json.load(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0BrWocEOgZzw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Showing a batch of images after applying the transforms on them"
      ]
    },
    {
      "metadata": {
        "id": "6o_LAV2A_HKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "def imshow(image):\n",
        "    if isinstance(image, torch.Tensor):\n",
        "        image = image.numpy().transpose((1, 2, 0))\n",
        "    else:\n",
        "        image = np.array(image).transpose((1, 2, 0))\n",
        "    # Unnormalize\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
        "    plt.imshow(image)\n",
        "    ax.axis('off') \n",
        "        \n",
        "images, _ = next(iter(dataloaders['train']))\n",
        "out = torchvision.utils.make_grid(images, nrow=8)\n",
        "imshow(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEc4kvF1gi2v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using pre-trained models ...... To be continued!"
      ]
    },
    {
      "metadata": {
        "id": "6EQ-7OApgpmI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}